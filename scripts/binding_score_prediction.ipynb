{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QMoeBQnUCK_E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.39.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#@title Install requirements. { display-mode: \"form\" }\n",
    "# Install requirements\n",
    "!pip install torch transformers sentencepiece h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tRe7CfuqFFmY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘protT5’: File exists\n",
      "mkdir: cannot create directory ‘protT5/protT5_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/sec_struct_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/output’: File exists\n",
      "File ‘protT5/example_seqs.fasta’ already there; not retrieving.\n",
      "\n",
      "File ‘protT5/sec_struct_checkpoint/secstruct_checkpoint.pt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Set up working directories and download files/checkpoints. { display-mode: \"form\" }\n",
    "# Create directory for storing model weights (2.3GB) and example sequences.\n",
    "# Here we use the encoder-part of ProtT5-XL-U50 in half-precision (fp16) as\n",
    "# it performed best in our benchmarks (also outperforming ProtBERT-BFD).\n",
    "# Also download secondary structure prediction checkpoint to show annotation extraction from embeddings\n",
    "!mkdir protT5 # root directory for storing checkpoints, results etc\n",
    "!mkdir protT5/protT5_checkpoint # directory holding the ProtT5 checkpoint\n",
    "!mkdir protT5/sec_struct_checkpoint # directory storing the supervised classifier's checkpoint\n",
    "!mkdir protT5/output # directory for storing your embeddings & predictions\n",
    "!wget -nc -P protT5/ https://rostlab.org/~deepppi/example_seqs.fasta\n",
    "# Huge kudos to the bio_embeddings team here! We will integrate the new encoder, half-prec ProtT5 checkpoint soon\n",
    "!wget -nc -P protT5/sec_struct_checkpoint http://data.bioembeddings.com/public/embeddings/feature_models/t5/secstruct_checkpoint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.sparkmagic',\n",
       " '.ipynb_checkpoints',\n",
       " 'protT5',\n",
       " '.virtual_documents',\n",
       " '.Trash-1000',\n",
       " 'PDB_homo.csv',\n",
       " 'embed_ProtT5 (1).ipynb',\n",
       " 'lost+found',\n",
       " 'embeddings.csv',\n",
       " 'PDB_transcrit.csv',\n",
       " 'PDB_prot.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"home/ec2-user/SageMaker/protT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZQotVM94S7NR"
   },
   "outputs": [],
   "source": [
    "# In the following you can define your desired output. Current options:\n",
    "# per_residue embeddings\n",
    "# per_protein embeddings\n",
    "# secondary structure predictions\n",
    "\n",
    "# Replace this file with your own (multi-)FASTA\n",
    "# Headers are expected to start with \">\";\n",
    "\n",
    "# whether to retrieve embeddings for each residue in a protein\n",
    "# --> Lx1024 matrix per protein with L being the protein's length\n",
    "# as a rule of thumb: 1k proteins require around 1GB RAM/disk\n",
    "per_residue = False\n",
    "per_residue_path = \"./protT5/output/per_residue_embeddings.h5\" # where to store the embeddings\n",
    "\n",
    "# whether to retrieve per-protein embeddings\n",
    "# --> only one 1024-d vector per protein, irrespective of its length\n",
    "per_protein = True\n",
    "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n",
    "\n",
    "# whether to retrieve secondary structure predictions\n",
    "# This can be replaced by your method after being trained on ProtT5 embeddings\n",
    "sec_struct = False\n",
    "sec_struct_path = \"./protT5/output/ss3_preds.fasta\" # file for storing predictions\n",
    "\n",
    "# make sure that either per-residue or per-protein embeddings are stored\n",
    "assert per_protein is True or per_residue is True or sec_struct is True, print(\n",
    "    \"Minimally, you need to active per_residue, per_protein or sec_struct. (or any combination)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET2v51slC5ui",
    "outputId": "15fa3a38-55dc-4e06-c1e2-5e6783df8456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "#@title Import dependencies and check whether GPU is available. { display-mode: \"form\" }\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c5XqIyeNStZP"
   },
   "outputs": [],
   "source": [
    "#@title Network architecture for secondary structure prediction. { display-mode: \"form\" }\n",
    "# Convolutional neural network (two convolutional layers) to predict secondary structure\n",
    "class ConvNet( torch.nn.Module ):\n",
    "    def __init__( self ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # This is only called \"elmo_feature_extractor\" for historic reason\n",
    "        # CNN weights are trained on ProtT5 embeddings\n",
    "        self.elmo_feature_extractor = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( 1024, 32, kernel_size=(7,1), padding=(3,0) ), # 7x32\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Dropout( 0.25 ),\n",
    "                        )\n",
    "        n_final_in = 32\n",
    "        self.dssp3_classifier = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( n_final_in, 3, kernel_size=(7,1), padding=(3,0)) # 7\n",
    "                        )\n",
    "\n",
    "        self.dssp8_classifier = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( n_final_in, 8, kernel_size=(7,1), padding=(3,0))\n",
    "                        )\n",
    "        self.diso_classifier = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( n_final_in, 2, kernel_size=(7,1), padding=(3,0))\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward( self, x):\n",
    "        # IN: X = (B x L x F); OUT: (B x F x L, 1)\n",
    "        x = x.permute(0,2,1).unsqueeze(dim=-1)\n",
    "        x         = self.elmo_feature_extractor(x) # OUT: (B x 32 x L x 1)\n",
    "        d3_Yhat   = self.dssp3_classifier( x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 3)\n",
    "        d8_Yhat   = self.dssp8_classifier( x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 8)\n",
    "        diso_Yhat = self.diso_classifier(  x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 2)\n",
    "        return d3_Yhat, d8_Yhat, diso_Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YLYFPAT_VLAK"
   },
   "outputs": [],
   "source": [
    "#@title Load the checkpoint for secondary structure prediction. { display-mode: \"form\" }\n",
    "def load_sec_struct_model():\n",
    "    checkpoint_dir=\"./protT5/sec_struct_checkpoint/secstruct_checkpoint.pt\"\n",
    "    state = torch.load( checkpoint_dir )\n",
    "    model = ConvNet()\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    print('Loaded sec. struct. model from epoch: {:.1f}'.format(state['epoch']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tLDz6jv1C0UI"
   },
   "outputs": [],
   "source": [
    "#@title Load encoder-part of ProtT5 in half-precision. { display-mode: \"form\" }\n",
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50)\n",
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/home/ec2-user/SageMaker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv \n",
    "\n",
    "homo = {}\n",
    "prot = {}\n",
    "transcrit = {}\n",
    "\n",
    "with open('PDB_homo.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    homo = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "with open('PDB_prot.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    prot = {rows[0]:rows[1] for rows in reader}\n",
    "    \n",
    "with open('PDB_transcrit.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    transcrit = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "del homo[\"PDB_File_Name\"]\n",
    "del prot[\"PDB_File_Name\"]\n",
    "del transcrit[\"PDB_File_Name\"]\n",
    "\n",
    "seqs = {}\n",
    "\n",
    "seqs.update(homo)\n",
    "seqs.update(prot)\n",
    "seqs.update(transcrit)\n",
    "\n",
    "print(len(seqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nK4hwGggR_Rs"
   },
   "outputs": [],
   "source": [
    "#@title Generate embeddings. { display-mode: \"form\" }\n",
    "# Generate embeddings via batch-processing\n",
    "# per_residue indicates that embeddings for each residue in a protein should be returned.\n",
    "# per_protein indicates that embeddings for a whole protein should be returned (average-pooling)\n",
    "# max_residues gives the upper limit of residues within one batch\n",
    "# max_seq_len gives the upper sequences length for applying batch-processing\n",
    "# max_batch gives the upper number of sequences per batch\n",
    "def get_embeddings( model, tokenizer, seqs, per_residue, per_protein, sec_struct,\n",
    "                   max_residues=4000, max_seq_len=1000, max_batch=100 ):\n",
    "\n",
    "    if sec_struct:\n",
    "      sec_struct_model = load_sec_struct_model()\n",
    "\n",
    "    results = {\"residue_embs\" : dict(),\n",
    "               \"protein_embs\" : dict(),\n",
    "               \"sec_structs\" : dict()\n",
    "               }\n",
    "\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
    "    seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
    "    start = time.time()\n",
    "    batch = list()\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n",
    "        seq = seq\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "\n",
    "        # count residues in current batch and add the last sequence length to\n",
    "        # avoid that batches with (n_res_batch > max_residues) get processed\n",
    "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len\n",
    "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = list()\n",
    "\n",
    "            # add_special_tokens adds extra token at the end of each sequence\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
    "                continue\n",
    "\n",
    "            if sec_struct: # in case you want to predict secondary structure from embeddings\n",
    "              d3_Yhat, d8_Yhat, diso_Yhat = sec_struct_model(embedding_repr.last_hidden_state)\n",
    "\n",
    "\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
    "                s_len = seq_lens[batch_idx]\n",
    "                # slice off padding --> batch-size x seq_len x embedding_dim\n",
    "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
    "                if sec_struct: # get classification results\n",
    "                    results[\"sec_structs\"][identifier] = torch.max( d3_Yhat[batch_idx,:s_len], dim=1 )[1].detach().cpu().numpy().squeeze()\n",
    "                if per_residue: # store per-residue embeddings (Lx1024)\n",
    "                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n",
    "                if per_protein: # apply average-pooling to derive per-protein embeddings (1024-d)\n",
    "                    protein_emb = emb.mean(dim=0)\n",
    "                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    passed_time=time.time()-start\n",
    "    avg_time = passed_time/len(results[\"residue_embs\"]) if per_residue else passed_time/len(results[\"protein_embs\"])\n",
    "    print('\\n############# EMBEDDING STATS #############')\n",
    "    print('Total number of per-residue embeddings: {}'.format(len(results[\"residue_embs\"])))\n",
    "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
    "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
    "        passed_time/60, avg_time ))\n",
    "    print('\\n############# END #############')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UB6yhwunTymY"
   },
   "outputs": [],
   "source": [
    "#@title Write embeddings to disk. { display-mode: \"form\" }\n",
    "def save_embeddings(emb_dict,out_path):\n",
    "    with h5py.File(str(out_path), \"w\") as hf:\n",
    "        for sequence_id, embedding in emb_dict.items():\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "juXcP5Tpbeqv"
   },
   "outputs": [],
   "source": [
    "#@title Write predictions to disk. { display-mode: \"form\" }\n",
    "def write_prediction_fasta(predictions, out_path):\n",
    "  class_mapping = {0:\"H\",1:\"E\",2:\"L\"}\n",
    "  with open(out_path, 'w+') as out_f:\n",
    "      out_f.write( '\\n'.join(\n",
    "          [ \">{}\\n{}\".format(\n",
    "              seq_id, ''.join( [class_mapping[j] for j in yhat] ))\n",
    "          for seq_id, yhat in predictions.items()\n",
    "          ]\n",
    "            ) )\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/home/ec2-user/SageMaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.sparkmagic',\n",
       " '.ipynb_checkpoints',\n",
       " 'protT5',\n",
       " '.virtual_documents',\n",
       " '.Trash-1000',\n",
       " 'PDB_homo.csv',\n",
       " 'embed_ProtT5 (1).ipynb',\n",
       " 'lost+found',\n",
       " 'embeddings.csv',\n",
       " 'PDB_transcrit.csv',\n",
       " 'PDB_prot.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()\n",
    "# os.chdir(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sec_struct_checkpoint',\n",
       " '.ipynb_checkpoints',\n",
       " 'protT5_checkpoint',\n",
       " 'example_seqs.fasta',\n",
       " 'output']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"protT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UKa6VWeE6kC",
    "outputId": "f9bbc5bd-2ab4-4ef5-a1f0-73ccddb2c722"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1713951086.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    seqs =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder part of ProtT5-XL-U50 in half-precision (recommended)\n",
    "model, tokenizer = get_T5_model()\n",
    "\n",
    "# Load example fasta.\n",
    "# seqs already defined\n",
    "\n",
    "# Compute embeddings and/or secondary structure predictions\n",
    "results = get_embeddings( model, tokenizer, seqs,\n",
    "                         per_residue, per_protein, sec_struct)\n",
    "\n",
    "# Store per-residue embeddings\n",
    "if per_residue: \n",
    "    save_embeddings(results[\"residue_embs\"], per_residue_path)\n",
    "if per_protein:\n",
    "    save_embeddings(results[\"protein_embs\"], per_protein_path)\n",
    "if sec_struct:\n",
    "    write_prediction_fasta(results[\"sec_structs\"], sec_struct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_dict_to_csv(dictionary, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for key, value in dictionary.items():\n",
    "            writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_filename = 'embeddings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dict_to_csv(results[\"protein_embs\"], csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(results[\"protein_embs\"])\n",
    "results = results[\"protein_embs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = {'AF-Q92392-F1.pdb': '0.7214553736978107', 'AF-Q04711-F1.pdb': '0.7705930843949318', 'AF-P10978-F1.pdb': '0.7672958493232727', 'AF-A0A5A7R4Q0-F1.pdb': '0.6621873825788498', 'AF-A0A6A3BVT5-F1.pdb': '0.6545961101849874', 'AF-P0CX58-F1.pdb': '0.7776032023959689', 'AF-A0A438G7Q2-F1.pdb': '0.7801340222358704', 'AF-A0A6L2J9H3-F1.pdb': '0.7836335102717081', 'AF-Q03855-F1.pdb': '0.8011221754550933', 'AF-Q12112-F1.pdb': '0.6529602408409119', 'AF-W8C110-F1.pdb': '0.7396126687526703', 'AF-W8AQS8-F1.pdb': '0.6627489262157016', 'AF-Q07163-F1.pdb': '0.7135160702925462', 'AF-P0C2J0-F1.pdb': '0.5091674961149693', 'AF-Q12441-F1.pdb': '0.7415428899583363', 'AF-A0A5N5HA23-F1.pdb': '0.5554636955261231', 'AF-Q12491-F1.pdb': '0.5969818112525073', 'AF-Q12316-F1.pdb': '0.5762644706604382', 'AF-P0CX60-F1.pdb': '0.6828132016318185', 'AF-P0CX63-F1.pdb': '0.6120331010648182', 'AF-A0A438CLY0-F1.pdb': '0.6917162656784057', 'AF-Q12088-F1.pdb': '0.6822161291326795', 'AF-A0A1D8BJE8-F1.pdb': '0.14345672726631165', 'AF-P0C2I9-F1.pdb': '0.5716737593923297', 'AF-A0A2L2YWJ1-F1.pdb': '0.6026824116706848', 'AF-P0CX76-F1.pdb': '0.7592370007187128', 'AF-P0CX69-F1.pdb': '0.7296085506677628', 'AF-Q04706-F1.pdb': '0.7369550606783699'}\n",
    "scores_test = {'AF-Q99231-F1.pdb': '0.6667538111408552', 'AF-Q12470-F1.pdb': '0.8035197721587287', 'AF-A0A438CP42-F1.pdb': '0.6251117587089539', 'AF-Q03494-F1.pdb': '0.7260885124023144', 'AF-P47099-F1.pdb': '0.6997073143720627', 'AF-Q04214-F1.pdb': '0.7640360531054045', 'AF-W8B9S8-F1.pdb': '0.7416476011276245', 'AF-Q03434-F1.pdb': '0.5454278354133878', 'AF-O13535-F1.pdb': '0.6057061470217175', 'AF-A0A438EZQ7-F1.pdb': '0.5015981793403625', 'AF-Q12472-F1.pdb': '0.612123923169242', 'AF-P0CX64-F1.pdb': '0.739402124285698', 'AF-P25384-F1.pdb': '0.6220023274421692', 'AF-W8AK63-F1.pdb': '0.6100412011146545', 'AF-A0A1D8BJE1-F1.pdb': '0.5655360221862793', 'AF-O13527-F1.pdb': '0.64699936658144', 'AF-Q07793-F1.pdb': '0.6031542474573309', 'AF-P0C2J3-F1.pdb': '0.5656805858016014'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "score_train_array = []\n",
    "embeddings_train = []\n",
    "\n",
    "for key in results:\n",
    "    # Check if the key exists in both dictionaries\n",
    "    if key in scores_train:\n",
    "        # Append score to score_array\n",
    "        score_train_array.append(float(scores_train[key]))\n",
    "        # Append numerical array to numerical_array\n",
    "        embeddings_train.append(results[key])\n",
    "\n",
    "# Convert numerical_array to numpy array\n",
    "embeddings_train = np.array(embeddings_train)\n",
    "\n",
    "# Print or use the arrays as needed\n",
    "print(\"Score Train Array:\", score_train_array)\n",
    "print(\"Embeddings Array:\", embeddings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(score_train_array), len(embeddings_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "score_test_array = []\n",
    "embeddings_test = []\n",
    "\n",
    "for key in results:\n",
    "    # Check if the key exists in both dictionaries\n",
    "    if key in scores_test:\n",
    "        # Append score to score_array\n",
    "        score_test_array.append(float(scores_test[key]))\n",
    "        # Append numerical array to numerical_array\n",
    "        embeddings_test.append(results[key])\n",
    "\n",
    "# Convert numerical_array to numpy array\n",
    "embeddings_test = np.array(embeddings_test)\n",
    "\n",
    "# Print or use the arrays as needed\n",
    "print(\"Score Test Array:\", score_test_array)\n",
    "print(\"Embeddings test Array:\", embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(score_test_array), len(embeddings_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "# Assuming you have embeddings and scores as numpy arrays\n",
    "# Convert them to PyTorch tensors\n",
    "embeddings_train = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "outputs = torch.tensor(score_train_array, dtype=torch.float32)\n",
    "\n",
    "print(len(embeddings_train), len(outputs))\n",
    "print(embeddings_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_dim = embeddings_train.shape[1]  # Dimension of input embeddings\n",
    "print(input_dim)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "step_size = 80  # Step size for learning rate scheduler\n",
    "gamma = 0.1  # Factor by which to reduce learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your model\n",
    "input_dim = 1024  # Dimension of input features\n",
    "output_dim = 1  # Dimension of output (1 for regression)\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "dataset = TensorDataset(embeddings_train, outputs)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = torch.tensor(embeddings_test, dtype=torch.float32)\n",
    "outputs_test = torch.tensor(score_test_array, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset for the test data\n",
    "test_dataset = TensorDataset(embeddings_test, outputs_test)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store training and testing accuracy for plotting\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for embeddings_batch, scores_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings_batch)\n",
    "        \n",
    "        # Ensure outputs has the correct shape\n",
    "        outputs = outputs.squeeze().view(-1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = outputs.detach().numpy()\n",
    "        real_scores = scores_batch.numpy()\n",
    "        correct_train_predictions += sum(abs(predictions - real_scores) < 0.05)\n",
    "        total_train_samples += len(predictions)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        loss = criterion(outputs, scores_batch)\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate training accuracy for the epoch\n",
    "    epoch_train_accuracy = correct_train_predictions / total_train_samples\n",
    "    train_accuracy_list.append(epoch_train_accuracy)\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_train_loss /= len(dataloader)\n",
    "    \n",
    "    # Testing phase\n",
    "    correct_test_predictions = 0\n",
    "    total_test_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings_test_batch, scores_test_batch in test_dataloader:\n",
    "            outputs_test = model(embeddings_test_batch)\n",
    "            outputs_test = outputs_test.squeeze().view(-1)\n",
    "            \n",
    "            # Calculate accuracy on test data\n",
    "            predictions_test = outputs_test.detach().numpy()\n",
    "            real_scores_test = scores_test_batch.numpy()\n",
    "            correct_test_predictions += sum(abs(predictions_test - real_scores_test) < 0.1)\n",
    "            total_test_samples += len(predictions_test)\n",
    "    \n",
    "    # Calculate testing accuracy for the epoch\n",
    "    epoch_test_accuracy = correct_test_predictions / total_test_samples\n",
    "    test_accuracy_list.append(epoch_test_accuracy)\n",
    "    \n",
    "    # Print epoch accuracy and loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Accuracy: {epoch_train_accuracy:.4f}, Training Loss: {epoch_train_loss:.4f}, Testing Accuracy: {epoch_test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training and testing accuracy\n",
    "plt.plot(range(1, num_epochs+1), train_accuracy_list, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), test_accuracy_list, label='Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
